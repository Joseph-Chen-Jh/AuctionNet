import pandas as pdimport numpy as npimport picklefrom functools import partialimport timeimport osfrom typing import List, Dict, Optional, Tuplefrom simul_bidding_env.PvGenerator.model_utils.model_utils import get_key_list# Get the full path of the current filecurrent_file_path = os.path.abspath(__file__)# Get the directory path of the current filecurrent_directory_path = os.path.dirname(current_file_path)# Constant definitionsPV_KEY_PATH = f'{current_directory_path}/stats_useful/pv_useful_key_list.txt'AD_KEY_PATH = f'{current_directory_path}/stats_useful/ad_key_list.txt'PV_SPLIT_KEY_LIST = ['id_info', 'pay_info', 'phone_info']GEO_KEY_MAX_LIST = ['prov_name_max', 'city_name_max', 'county_name_max']GEO_KEY_LIST = ['prov_name', 'city_name', 'county_name']MINUTE_MAP_DICT = {i: i // 15 for i in range(60)}# Helper functionsdef time_map(row: pd.Series, additional_name: Optional[str] = None, minute_map_dict: Dict[int, int] = MINUTE_MAP_DICT, hour_coeff: int = 4) -> int:    """    Generate time mapping.    Args:        row (pd.Series): Row data.        additional_name (Optional[str]): Additional name.        minute_map_dict (Dict[int, int]): Minute mapping dictionary.        hour_coeff (int): Hour coefficient.    Returns:        int: Time index.    """    key = 'time_stamp'    if additional_name is not None:        key = f'{key}_{additional_name}'    unix_time = row[key]    date_time = time.gmtime(unix_time)    minute = int(date_time.tm_min)    hour = int(date_time.tm_hour)    idx = hour * hour_coeff + minute_map_dict[minute]    return idxdef dict_map(x, dict, mode=1, dict2=None):    """    Dictionary mapping.    Args:        x: Input value.        dict: Dictionary.        mode (int): Mode.        dict2: Second dictionary.    Returns:        str: Mapped value.    """    if x not in dict:        y = 'default'    else:        y = x    if mode == 1:        return dict[y]    else:        return dict2[dict[y]]def build_key(additional_name: Optional[str] = None, key_type: str = 'user') -> List[str]:    """    Build keys.    Args:        additional_name (Optional[str]): Additional name.        key_type (str): Key type.    Returns:        List[str]: List of keys.    """    pv_key_list = get_key_list(PV_KEY_PATH, split_key=True)    ad_key_list = get_key_list(AD_KEY_PATH)    all_pv_key_list = []    for split_key in PV_SPLIT_KEY_LIST:        all_pv_key_list += pv_key_list[split_key]    if key_type == 'all':        all_pv_key_list += ad_key_list    if additional_name is not None:        for i in range(len(all_pv_key_list)):            all_pv_key_list[i] += f'_{additional_name}'    return all_pv_key_listdef num_to_decimal(x: int) -> np.ndarray:    """    Convert number to decimal array.    Args:        x (int): Input number.    Returns:        np.ndarray: Decimal array.    """    ret = []    while x > 0:        r = x % 10        ret.append(r)        x //= 10    len_r = len(ret)    for i in range(len_r, 6):        ret.append(0)    ret.reverse()    return np.array(ret, dtype=int)def build_data_from_df(all_df: pd.DataFrame, additional_name: Optional[str] = None, key_map_dict_name: str = 'str_key_map_dict.pkl', key_type: str = 'user', normalize: str = 'minmax', ori_info_dict: Optional[Dict] = None, stats_table: Optional[pd.DataFrame] = None, args = None) -> Tuple[np.ndarray, int, Dict]:    """    Build data from DataFrame.    Args:        all_df (pd.DataFrame): DataFrame.        additional_name (Optional[str]): Additional name.        key_map_dict_name (str): Key mapping dictionary name.        key_type (str): Key type.        normalize (str): Normalization type.        ori_info_dict (Optional[Dict]): Original information dictionary.        stats_table (Optional[pd.DataFrame]): Statistics table.        args: Arguments.    Returns:        Tuple[np.ndarray, int, Dict]: Data, dimension count, and information dictionary.    """    all_pv_key_list = build_key(additional_name=additional_name, key_type=key_type)    if args.special_normalize:        special_normalize_key_list = args.special_normalize_key_list.split(',')    if args.pv_label_minmax:        pv_label_key_list = 'pctr_clbr,pcfr_clbr,pcvr_clbr'.split(',')    all_pv_key_list.append('zip_code')    key_map_dict_path = f'{current_directory_path}/stats_useful/{key_map_dict_name}'    with open(key_map_dict_path, 'rb') as f:        str_key_map_dict = pickle.load(f)    dim_count = 0    data_list = []    info_dict = {}    geo_judge = [f'{k}_{additional_name}' for k in GEO_KEY_LIST] if additional_name else GEO_KEY_LIST    for key in all_pv_key_list:        if key in geo_judge:            continue        str_key = key if additional_name else f'{key}_max'        if str_key not in str_key_map_dict:            if key == 'zip_code':                info_dict[key] = {'dtype': 'onehot'}                data = all_df['zip_code']                data = data.apply(num_to_decimal)                data = np.stack(data)                data = data.astype(int)                one_decimal_hot = 10                decimal_num = np.shape(data)[-1]                eye = np.eye(one_decimal_hot)                data = eye[data]                data = np.reshape(data, [-1, one_decimal_hot * decimal_num])            elif key.startswith('time_stamp'):                info_dict[key] = {'dtype': 'onehot'}                data = all_df.apply(time_map, axis=1)                data = np.array(data)                data = data.astype(int)                time_dim = 96                eye = np.eye(time_dim)                data = eye[data]            else:                continue        else:            info_dict[key] = {'dtype': 'onehot'}            str_dict1 = str_key_map_dict[str_key]['name2idx']            str_dict2 = str_key_map_dict[str_key]['idx2name']            if 'default' not in str_dict1:                str_dict1['default'] = 0            idv_dict_map = partial(dict_map, dict=str_dict1)            data = all_df[key].map(idv_dict_map)            data = np.array(data, dtype=np.int64)            onehot_dim = len(str_dict2)            eye = np.eye(onehot_dim)            data = eye[data]        data_dim = np.shape(data)[-1]        info_dict[key]['pos'] = [dim_count, dim_count + data_dim]        dim_count += data_dim        data_list.append(data)    for key in all_pv_key_list:        if key in geo_judge:            continue        str_key = key if additional_name else f'{key}_max'        if str_key not in str_key_map_dict and key != 'zip_code' and not key.startswith('time_stamp'):            info_dict[key] = {}            data = all_df[key].fillna(0)            data = np.array(data, dtype=np.float64)            if len(np.shape(data)) == 1:                data = np.expand_dims(data, axis=-1)            if ori_info_dict is None:                if args.special_normalize:                    if key in special_normalize_key_list:                        key_normalize = args.special_normalize_type                    else:                        key_normalize = normalize                else:                    if args.pv_label_minmax and key in pv_label_key_list:                        key_normalize = 'minmax'                    else:                        key_normalize = normalize                if key_normalize == 'minmax':                    if stats_table is None:                        info_data_min = np.min(data)                        info_data_max = np.max(data)                    else:                        info_data_min = stats_table[f'{key}_min'][0]                        info_data_max = stats_table[f'{key}_max'][0]                        if info_data_min == '\\N':                            info_data_min = np.min(data)                            info_data_max = np.max(data)                        else:                            info_data_min = float(info_data_min)                            info_data_max = float(info_data_max)                    key_shift = info_data_min                    key_scale = (info_data_max - key_shift)                elif key_normalize == 'mean':                    if stats_table is None or key not in stats_table:                        info_data_mean = np.mean(data)                        info_data_std = np.std(data)                    else:                        info_data_mean = stats_table[f'{key}_mean'][0]                        info_data_std = stats_table[f'{key}_std'][0]                        if info_data_mean == '\\N':                            info_data_mean = np.mean(data)                            info_data_std = np.std(data)                        else:                            info_data_mean = float(info_data_mean)                            info_data_std = float(info_data_std)                    key_shift = info_data_mean                    key_scale = info_data_std            else:                key_shift = ori_info_dict[key]['shift']                key_scale = ori_info_dict[key]['scale']            if key_scale == 0.0:                key_scale = 1            info_dict[key]['dtype'] = 'scalar'            info_dict[key]['shift'] = key_shift            info_dict[key]['scale'] = key_scale            data = (data - key_shift) / key_scale            if args.data_normalize_scale:                if args.special_normalize and key in special_normalize_key_list:                    print(f'skip data clip for key {key} for special_normalize')                elif args.pv_label_minmax and key in pv_label_key_list:                    print(f'skip data clip for key {key} for pv_label_minmax')                else:                    data = np.clip(data, a_min=-args.data_normalize_scale_value, a_max=args.data_normalize_scale_value)        else:            continue        data_dim = np.shape(data)[-1]        info_dict[key]['pos'] = [dim_count, dim_count + data_dim]        dim_count += data_dim        data_list.append(data)    data_all = np.concatenate(data_list, axis=-1)    return data_all, dim_count, info_dict